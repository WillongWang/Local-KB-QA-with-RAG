# Enhancing Local Knowledge Base QA with LLMs and RAG Integration

This project aims to enhance local knowledge base QA by integrating **Chinese LLMs** with **RAG** technology using **LangChain**. It enables users to upload unstructured documents and generates accurate, context-aware responses through similarity search. A **chatbot GUI** is built with **Gradio** to provide an intuitive user interface for interacting with the system. The project plans to evaluate various prompt strategies on the JD QA dataset and some literary texts using precision and ROUGE metrics for performance assessment.

The project follows the fundamental RAG pipeline, which involves several key steps. First, the uploaded documents are split into smaller chunks based on predefined rules, making them easier to process. Next, these chunks are embedded and vectorized to facilitate similarity comparison. When a user submits a query, the system performs a similarity search on the embedded document vectors to identify the most relevant passages. The retrieved content is then combined with the user’s query to generate a coherent and accurate response.

## Chinese LLMs

1.CHATGLM

Deploy ChatGLM-6B according to the [official website](https://github.com/THUDM/ChatGLM-6B). I deployed it locally on Windows using the API and directly loaded the quantized model. The INT4 quantized model only requires about 5.2GB of memory. Model quantization may lead to some performance loss, but through testing, ChatGLM-6B can still generate natural and fluent responses with 4-bit quantization. You are recommended to download the model directly from Hugging Face ([chatglm-6b](https://huggingface.co/THUDM/chatglm-6b/tree/main) or [chatglm-6b-int4](https://huggingface.co/THUDM/chatglm-6b-int4/tree/main)) and remember to install the GCC for the INT4 model.

```
# Replace the corresponding part in the original official api.py, where self-chatglm-6b-int4 is your local chatglm-6b or chatglm-6b-int4 directory
tokenizer = AutoTokenizer.from_pretrained("self-chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("self-chatglm-6b-int4", trust_remote_code=True).half().cuda()
model.eval()  
```

Then, deploy the ChatGLM model:

```
# first create a separate virtual environment for deploying
pip install -r requirements_chatglm.txt
python api.py
```

Create the LLM using the following approach in chatglm_document_qa.py:

```
from langchain_community.llms import ChatGLM

llm = ChatGLM(
     endpoint_url='http://127.0.0.1:8000',
     max_token=80000,
     top_p=0.9)
```

2.DeepSeek

To balance time and cost, I also used the DeepSeek API. According to the official website, the deepseek-chat model has been fully upgraded to DeepSeek-V3.

```
from langchain_deepseek import ChatDeepSeek

llm=ChatDeepSeek(
    model="deepseek-chat",
    temperature=0.7,
    max_tokens=2000,
    api_key="") #Your api_key
```

## How to run

```
pip install -r requirements.txt
python chatglm_document_qa.py
```

Default settings:

You can adjust them accordingly.

```
#chunk_size & chunk_overlap to split the original or uploaded text
text_spliter = CharacterTextSplitter(chunk_size=256, chunk_overlap=0) 

# prompt
QA_CHAIN_PROMPT = PromptTemplate.from_template("""根据以下已知信息回答问题：
{context}
问题：{question}
请用中文简洁回答，如果无法回答请说不知道。""")

retriever = db.as_retriever(search_kwargs={"k": 3}) #return 3 relevant docs
```

This file by default processes a small JD Q&A dataset [jd_faq.csv](https://github.com/WillongWang/Awesome-LLM-NLP-projects-updating-/blob/main/Local-KB-QA-with-RAG/documents/jd_faq.csv) as an experiment.  
Each line is used to construct a langchain.schema.Document object and is then vectorized and stored. You can comment out the corresponding code snippet if needed.

### ROUGE

This project uses ROUGE to evaluate the QA performance. Each record in [jd_faq.csv](https://github.com/WillongWang/Awesome-LLM-NLP-projects-updating-/blob/main/Local-KB-QA-with-RAG/documents/jd_faq.csv) contains a question and its corresponding answer. The LLM is prompted to answer each question, and the generated answer is compared with the original one to compute the ROUGE score.

```
# pip install jieba rouge_chinese
python rouge.py
```

#### Necessary adjustments and unavoidable limitations:

1. The maximum supportable number of tokens generated by DeepSeek is smaller than the maximum length of answers in the dataset.

```
# The maximum record (line) length in jd_faq.csv: 15,290 characters, at line number 415
# Record length statistics:
# Minimum: 17
# Average: 353.41
# Median: 132.0
llm = ChatDeepSeek( 
    model="deepseek-chat",
    temperature=0.7,
    max_tokens=8000,  # Setting more than 8000 will result in an error
    api_key=""
)
```

Actually, for the question with the longest answer (line 415: "国际机票中国国际航空旅客、行李运输须知"), the model generated the following response:  
"国际机票中国国际航空旅客、行李运输须知的原文内容如下： ...(original text) （后续条款因篇幅限制未完整列出，完整内容请参考中国国际航空官方文件。）
如需具体某一条款的详细内容，请告知具体条款编号。"  
This showcases the LLM’s strong QA capabilities — it correctly generated an appropriate introduction and thoughtfully indicated that some clauses were omitted due to length. However, since ROUGE-N precision mainly captures surface-level word overlap, it may underrate the model’s true ability and fail to reflect its actual understanding and reasoning.

2. To make the LLM answer more accurately, it is explicitly instructed to respond based on the provided context:

```
QA_CHAIN_PROMPT = PromptTemplate.from_template("""根据以下已知信息回答问题：
{context}
问题：{question}
请用原文回答，如果无法回答请说不知道。""")
```

3. The LLM sometimes prepends "答案:" ("Answer:") to its responses, so post-processing is applied to remove that prefix:

```
def clean_answer(text):
    if text.startswith('答案:'):
        return text[3:]
    return text
```

#### Results

| Version              | ROUGE-1 (Recall) | ROUGE-1 (Precision) | ROUGE-1 (F1) | ROUGE-2 (R) | ROUGE-2 (P) | ROUGE-2 (F) | ROUGE-L (R) | ROUGE-L (P) | ROUGE-L (F) |
|----------------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|
| Original             | 0.8698      | 0.8820      | 0.8622      | 0.8572      | 0.8492      | 0.8485      | 0.8636      | 0.8767      | 0.8560      |
| With `clean_answer()`| 0.8697      | 0.8924      | 0.8677      | 0.8572      | 0.8588      | 0.8538      | 0.8636      | 0.8852      | 0.8605      |

## Running Examples

![](https://github.com/WillongWang/Awesome-LLM-NLP-projects-updating-/blob/main/Local-KB-QA-with-RAG/1.png)

![](https://github.com/WillongWang/Awesome-LLM-NLP-projects-updating-/blob/main/Local-KB-QA-with-RAG/2.png)

### Bugs

Currently, only document uploads from a folder are supported, and all files in the folder will be fully uploaded, because:

```
# Not compatible with ChatDeepSeek
loader = TextLoader(directory, encoding='utf-8')

# Must use 
loader = DirectoryLoader(directory)
```









